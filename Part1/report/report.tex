
\documentclass[11pt, a4paper]{article}

% --- 1. PREAMBLE ---
% --- Geometry & Font ---
\usepackage[a4paper, top=2.5cm, bottom=2.5cm, left=2.5cm, right=2.5cm]{geometry}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage{lmodern}
\renewcommand{\familydefault}{\sfdefault}

% --- Standard Packages ---
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{verbatim}
\usepackage{textcomp}

% --- Section Styling ---
\usepackage{sectsty}
\sectionfont{\sffamily\Large}
\subsectionfont{\sffamily\large}
\subsubsectionfont{\sffamily\normalsize}

% --- Hyperlinks ---
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=cyan,
    pdftitle={GEMM Optimization Report},
    pdfauthor={Arpit Prasad and Devansh Pandey},
}

% --- 2. TITLE SECTION ---
\title{
    \sffamily\bfseries\huge
    Lab: Accelerating Code: From Python Baseline to Architecture-Aware Optimization \\
    \vspace{0.5\baselineskip}
    \large Dense Matrix-Matrix Multiplication (GEMM) Challenge
}

\author{
    \sffamily
    Arpit Prasad and Devansh Pandey \\
    2022EE11837 and 2022EE31538
}

\date{\today}

% --- 3. DOCUMENT BODY ---
\begin{document}

\maketitle

% --- Section 1 ---
\section{Introduction}
This report documents our efforts to accelerate dense matrix--matrix multiplication (GEMM) starting from a Python baseline and building up to an architecture-aware optimized C++ implementation. Our design incorporates cache blocking, AVX2/AVX-512 vectorization, NUMA-aware allocation, and multi-threading with OpenMP.

% --- Section 2 ---
\section{Baseline Implementation}
The baseline implementation is a pure Python/NumPy version using the in-built highly optimized \texttt{np.dot()} routine. While NumPy internally dispatches to BLAS, for comparison in this assignment we treat it as the baseline reference.

Our environment:

\begin{itemize}
    \item \textbf{CPU Model:} 13th Gen Intel(R) Core(TM) i5-1340P
    \item \textbf{Cores/Threads:} 2 threads per core, 12 cores
    \item \textbf{Caches:} L1d/L1i/L2/L3: 448KiB/640KiB/9MiB/12MiB
    \item \textbf{OS:} Ubuntu 22.04 LTS
    \item \textbf{Compiler:} g++ 17
\end{itemize}

The baseline is single-threaded from Python's perspective and does not exploit explicit parallelism or architecture-aware optimizations.

% --- Section 3 ---
\section{Optimizations Implemented}
We implemented the following optimizations.

\subsection{Multithreading with OpenMP}
We parallelized the matrix multiplication using OpenMP. We set thread affinity via:
\begin{itemize}
    \item \texttt{OMP\_PROC\_BIND=TRUE}
    \item \texttt{OMP\_PLACES=cores}
\end{itemize}
This reduces OS scheduling overhead and improves cache locality.

\subsection{Cache-Friendly Blocking}
We implemented a 3-level cache blocking strategy with tunable block sizes (MC, KC, NC). Blocking ensures that reused submatrices remain in L1/L2 cache.

\subsection{Vectorization: AVX2 and AVX-512}
Specialized compute kernels were written using 256-bit (AVX2) and 512-bit (AVX-512) intrinsics. The program automatically detects hardware capabilities using \texttt{\_\_builtin\_cpu\_supports} and selects the best available kernel.

\subsection{NUMA-Aware Memory Initialization}
By performing parallel first-touch initialization, each thread physically allocates its portion of the memory on the NUMA node it executes on. This reduces remote memory accesses.

\subsection{Parallel Transposition of B}
We transpose matrix B into a contiguous, cache-friendly layout B\_T. This greatly improves spatial locality when accessing B along k.

\subsection{Aligned Memory Allocation}
All matrices are allocated with 64-byte alignment using \texttt{posix\_memalign} to maximize AVX load/store efficiency.

% --- Section 4 ---
\section{Experimental Methodology}
We used automated scripts to run multiple experiments:

\begin{enumerate}
    \item One warmup run per configuration
    \item Ten measured runs per pair of (N, T)
    \item Automatic CSV logging via our \texttt{run.sh} and \texttt{run\_all\_tests.sh}
    \item Metrics collected: runtime, GFLOPS, speedup
\end{enumerate}

We tested matrix sizes: 512, 1024, 1536, 2048.

We tested thread counts: 1, 2, 4, 8, 16.

% --- Section 5 ---
\section{Performance Results}

\subsection{Summary Table}

\begin{table}[htbp]
\centering
\caption{Key Performance Metrics (Avg. of 10 Runs)}
\label{tab:results}
\begin{tabular}{lrrrr}
\toprule
\textbf{Metric} & \textbf{512} & \textbf{1024} & \textbf{1536} & \textbf{2048} \\
\midrule
Baseline Time (s) & [ ] & [ ] & [ ] & [ ] \\
Optimized Time (s, T=16) & [ ] & [ ] & [ ] & [ ] \\
Optimized GFLOPS & [ ] & [ ] & [ ] & [ ] \\
Max Speedup & [ ] & [ ] & [ ] & [ ] \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Time vs Matrix Size}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{/home/arpit/Desktop/iitd/sem_7/COL718/projects/Optimal-Mat-Mul/Part1/plots/plot_time_vs_size.png}
    \caption{Runtime vs Matrix Size for Optimized Kernel.}
\end{figure}

\subsection{GFLOPS vs Matrix Size}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{/home/arpit/Desktop/iitd/sem_7/COL718/projects/Optimal-Mat-Mul/Part1/plots/plot_gflops_vs_size.png}
    \caption{GFLOPS vs Matrix Size. Larger matrices amortize overhead and better utilize caches and SIMD.}
\end{figure}

\subsection{Speedup vs Matrix Size}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{/home/arpit/Desktop/iitd/sem_7/COL718/projects/Optimal-Mat-Mul/Part1/plots/plot_speedup_vs_size.png}
    \caption{Speedup (Baseline / Optimized) vs Matrix Size.}
\end{figure}

\subsection{GFLOPS Scaling with Threads}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{/home/arpit/Desktop/iitd/sem_7/COL718/projects/Optimal-Mat-Mul/Part1/plots/plot_gflops_vs_threads.png}
    \caption{GFLOPS vs Thread Count.}
\end{figure}

\subsection{Speedup Scaling with Threads}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{/home/arpit/Desktop/iitd/sem_7/COL718/projects/Optimal-Mat-Mul/Part1/plots/plot_speedup_vs_threads.png}
    \caption{Speedup vs Thread Count.}
\end{figure}

% --- Section 6 ---
\section{Discussion}
\subsection{Bottleneck Analysis}
TODO: Insert perf analysis (L1/L2/L3 miss rates, IPC, bandwidth usage).

\subsection{Limits to Scaling}
TODO: Discuss memory bandwidth saturation, AVX frequency downclock, NUMA effects.

% --- Section 7 ---
\section{Conclusion}
We achieved significant acceleration of GEMM using a combination of cache blocking, SIMD vectorization, NUMA-aware memory placement, and multi-threaded execution. Further improvements could involve dynamic block-size tuning, software prefetching, and a more advanced roofline-guided optimization strategy.

\end{document}
