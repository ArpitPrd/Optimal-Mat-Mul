% !TeX program = lualatex
%
% --- COMPILATION NOTE ---
% This template MUST be compiled with LuaLaTeX (or XeLaTeX).
% It will not work with pdflatex.
% This is because it uses the 'fontspec' package to load modern system fonts
% like 'Noto Sans' for better Unicode support.
%
\documentclass[11pt, a4paper]{article}

% --- UNIVERSAL PREAMBLE BLOCK ---
% Set page geometry
\usepackage[a4paper, top=2.5cm, bottom=2.5cm, left=2.5cm, right=2.5cm]{geometry}

% --- pdflatex-compatible font setup ---
\usepackage[T1]{fontenc} % Use T1 font encoding
\usepackage{lmodern}     % Use the Latin Modern font (good default)
\usepackage[utf8]{inputenc}  % Allow UTF-8 input
\usepackage[english]{babel}  % Set language for pdflatex

% Load font and language packages
% \usepackage{fontspec}
% \usepackage[english, bidi=basic, provide=*]{babel}

% Provide the main language (english)
% \babelprovide[import, onchar=ids fonts]{english}

% Set default/Latin font to Sans Serif in the main (rm) slot
% You can change "Noto Sans" to "Noto Serif" if you prefer a serif font
% \babelfont{rm}{Noto Sans}
% --- END UNIVERSAL PREAMBLE BLOCK ---

% --- Additional Packages for this Report ---

% For advanced math environments (like the recurrence relation)
\usepackage{amsmath}

% For professional-looking tables
\usepackage{booktabs}

% For colors in code listings
\usepackage{xcolor}

% For code listings (e.g., for compiler flags or perf output)
\usepackage{listings}
\lstset{
  basicstyle=\ttfamily\small,
  breaklines=true,
  backgroundcolor=\color{gray!10}, % A light gray background
  frame=single,
  framerule=0pt,
  showstringspaces=false,
}

% For (placeholder) figures
% We use \framebox for placeholders to ensure compilation without external files

% For hyperlinks (e.g., in the ToC and references)
% This MUST be the last package loaded
\usepackage[hidelinks]{hyperref}


% --- Document Information ---
\title{Lab Report: Accelerating the Smith-Waterman Algorithm}
\author{
  Student Name 1 (EntryNo1) \and
  Student Name 2 (EntryNo2)
}
\date{\today}

% =================================================================
% --- BEGIN DOCUMENT ---
% =================================================================
\begin{document}

\maketitle

\begin{abstract}
This report details the iterative optimization of the Smith-Waterman algorithm for local sequence alignment. Starting from a baseline Python implementation, we describe a series of architecture-aware optimizations, including C++ reimplementation, SIMD vectorization, and multicore parallelism. We present a detailed performance analysis, including speedup, scalability, and profiling data, to quantify the impact of each optimization. We conclude with a reflection on the most significant architectural factors that dominated performance.
\end{abstract}

\tableofcontents
\newpage

% =================================================================
\section{Introduction}
% =================================================================

The Smith-Waterman (S-W) algorithm is a fundamental algorithm in bioinformatics used for local sequence alignment. It finds the most similar regions between two biological sequences (e.g., DNA or protein) using a dynamic programming approach.

The objective of this assignment is to accelerate a provided Python baseline implementation of the S-W algorithm. We explore a series of optimizations targeting modern CPU architectures, moving from a high-level language to architecture-aware C++ code.

This report is structured as follows:
\begin{itemize}
    \item \textbf{Section 2} briefly describes the S-W algorithm and its recurrence relation.
    \item \textbf{Section 3} analyzes the baseline implementation and its performance.
    \item \textbf{Section 4} details each optimization strategy applied, from compiler flags to vectorization and multi-threading.
    \item \textbf{Section 5} presents the performance results, including speedup, scalability, and profiling evidence.
    \item \textbf{Section 6} provides a final reflection on the key performance bottlenecks and takeaways.
\end{itemize}

\subsection{System Environment}
All benchmarks were conducted on the following system. This is critical for reproducibility.

\begin{itemize}
    \item \textbf{CPU:} [e.g., Intel Core i9-XXXX or AMD Ryzen 9 XXXX]
    \item \textbf{Cores/Threads:} [e.g., 16 Cores / 32 Threads]
    \item \textbf{L1/L2/L3 Cache:} [e.g., 32KB/256KB/16MB]
    \item \textbf{SIMD Support:} [e.g., AVX2, AVX-512]
    \item \textbf{OS:} [e.g., Ubuntu 22.04 LTS]
    \item \textbf{Compiler:} [e.g., g++ 11.4.0]
    \item \textbf{Compiler Flags (Baseline):} [e.g., -O2]
\end{itemize}


% =================================================================
\section{The Smith-Waterman Algorithm}
% =================================================================

The core of the S-W algorithm is the computation of a scoring matrix $H$ where $H(i, j)$ is the maximum alignment score between a suffix of the first sequence (ending at $i$) and a suffix of the second sequence (ending at $j$).

The recurrence relation is defined as:
$$
H(i,j) = \max
\begin{cases}
    0 \\
    H(i-1, j-1) + s(a_i, b_j) \\
    H(i-1, j) + \text{gap} \\
    H(i, j-1) + \text{gap}
\end{cases}
$$
where $s(a_i, b_j)$ is the score for a match or mismatch, and $\text{gap}$ is the penalty for an insertion or deletion. For this assignment, the scoring scheme is:
\begin{itemize}
    \item \textbf{Match:} +2
    \item \textbf{Mismatch:} -1
    \item \textbf{Gap:} -2
\end{itemize}


% =================================================================
\section{Baseline Implementation and Performance}
% =================================================================

The baseline implementation is a pure Python script [Name of baseline script, e.g., \texttt{sw\_baseline.py}]. It implements the recurrence relation directly using nested loops to fill the DP matrix.

[Describe the input sequences used for benchmarking, e.g., synthetic DNA sequences of length N and M].

The wall-clock time for the baseline implementation on the test input was:
\begin{itemize}
    \item \textbf{Baseline Runtime:} [e.g., 120.5 seconds]
\end{itemize}
This runtime serves as the basis for all speedup calculations.


% =================================================================
\section{Optimization Strategies and Reasoning}
% =================================================================

We applied a series of optimizations iteratively, measuring the performance at each step.

\subsection{C++ Re-implementation (Serial)}
The first step was to port the algorithm from Python to C++. This eliminates the overhead of the Python interpreter and allows for more direct control over memory and execution.
\begin{itemize}
    \item \textbf{Reasoning:} Python's dynamic typing and interpreted nature introduce significant overhead for tight numerical loops. C++ is a compiled language that can generate much more efficient machine code.
    \item \textbf{Performance:} [Runtime and Speedup]
\end{itemize}

\subsection{Compiler Optimizations}
We compiled the C++ code with aggressive optimization flags.
\begin{itemize}
    \item \textbf{Flags Used:} \texttt{-O3 -march=native -funroll-loops}
    \item \textbf{Reasoning:} \texttt{-O3} enables a wide range of optimizations. \texttt{-march=native} allows the compiler to generate instructions specific to our CPU, including auto-vectorization. \texttt{-funroll-loops} can reduce branch overhead in loops.
    \item \textbf{Performance:} [Runtime and Speedup]
\end{itemize}

\subsection{Cache-Friendly Blocking (Tiling)}
The S-W algorithm has data dependencies that make simple blocking difficult. However, we can use techniques like wavefront parallelism or tiled computation.
\begin{itemize}
    \item \textbf{Reasoning:} [Explain your blocking strategy. The standard DP matrix computation is cache-unfriendly as it sweeps through large amounts of memory. Blocking aims to keep the "hot" working set in the cache.]
    \item \textbf{Diagram:} Below is a placeholder for a diagram illustrating our data blocking/tiling strategy.
\end{itemize}

\begin{figure}[htbp]
  \centering
  \framebox{\parbox{0.8\textwidth}{\centering
    \vspace{5cm}
    \textbf{Figure Placeholder: Data Blocking Strategy} \\
    \small\textit{A diagram showing how the DP matrix was tiled or processed
    in blocks to improve cache locality (e.t., wavefront).}
    \vspace{5cm}
  }}
  \caption{Illustration of the data blocking strategy.}
  \label{fig:blocking}
\end{figure}

\subsection{SIMD Vectorization (e.g., AVX2/AVX-512)}
We manually vectorized the innermost loop using AVX intrinsics.
\begin{itemize}
    \item \textbf{Reasoning:} The S-W recurrence involves multiple independent $\max$ operations, which are well-suited for SIMD. A single AVX-512 instruction can perform operations on 16 integers (32-bit) in parallel.
    \item \textbf{Implementation:} [Describe which intrinsics were key, e.g., \texttt{\_mm512\_max\_epi32}, \texttt{\_mm512\_add\_epi32}, \texttt{\_mm512\_loadu\_si512}, etc.]
    \item \textbf{Performance:} [Runtime and Speedup]
\end{itemize}

\subsection{Multicore Parallelism (Multi-threading)}
Finally, we parallelized the computation across multiple CPU cores using [e.g., OpenMP, std::thread, C++ Pthreads].
\begin{itemize}
    \item \textbf{Reasoning:} [Describe your parallelization strategy. For example, did you parallelize an outer loop? Did you use task-based parallelism for the wavefront blocks?]
    \item \textbf{Synchronization:} [Describe any synchronization needed, e.g., barriers, locks, or if it was embarrassingly parallel.]
    \item \textbf{Performance:} [Runtime and Speedup on N cores]
\end{itemize}


% =================================================================
\section{Performance Analysis and Results}
% =================================================================

This section quantifies the performance of each optimization.

\subsection{Overall Performance Improvement}
The table below summarizes the runtime and speedup at each stage of optimization.

\begin{table}[htbp]
  \centering
  \caption{Performance at each optimization step.}
  \label{tab:results}
  \begin{tabular}{@{}lrr@{}}
    \toprule
    \textbf{Optimization Step} & \textbf{Runtime (sec)} & \textbf{Speedup (over baseline)} \\
    \midrule
    1. Python Baseline      & [e.g., 120.50]   & 1.00x \\
    2. C++ Serial (O2)      & [e.g., 10.20]    & [e.g., 11.8x] \\
    3. C++ Serial (O3, native) & [e.g., 8.50]     & [e.g., 14.2x] \\
    4. + Blocking             & [e.g., 7.10]     & [e.g., 17.0x] \\
    5. + AVX-512              & [e.g., 2.30]     & [e.g., 52.4x] \\
    6. + Multicore (N cores)  & [e.g., 0.45]     & [e.g., 267.8x] \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{Scalability Analysis}
The plot below shows the speedup of the final parallel implementation as the number of threads increases from 1 to [Max Cores].

\begin{figure}[htbp]
  \centering
  \framebox{\parbox{0.8\textwidth}{\centering
    \vspace{6cm}
    \textbf{Figure Placeholder: Scalability Plot} \\
    \small\textit{A line graph with Threads on the X-axis and Speedup on the Y-axis.
    Show both ideal linear scaling and the measured scaling.}
    \vspace{6cm}
  }}
  \caption{Scalability: Speedup vs. Number of Threads.}
  \label{fig:scaling}
\end{figure}

\subsection{Profiling Evidence (Cache/IPC)}
To validate our reasoning, we used \texttt{perf} to profile the code.

\begin{itemize}
    \item \textbf{Cache Misses:} The cache-blocking optimization reduced L3 cache misses from [X\%] to [Y\%], demonstrating improved data locality.
    \item \textbf{IPC:} Manual vectorization with AVX-512 significantly improved the Instructions Per Cycle (IPC). The scalar C++ version had an IPC of [e.g., 0.8], while the vectorized version achieved an IPC of [e.g., 2.5].
\end{itemize}

Example \texttt{perf stat} output for the final optimized version:
\begin{lstlisting}[language=bash, caption={Perf stat output for optimized run}]
 Performance counter stats for './sw_optimized':

   1,393.94 msec    task-clock   # 1.887 CPUs utilized
   59             context-switches
   1,95,451       page-faults
   4,25,73,83,614 cycles       # 3.054 GHz
   4,48,34,89,980 instructions # 1.05 insn per cycle
   51,04,34,834   branches
   1,47,317       branch-misses  # 0.03% of all branches

   0.738770142 seconds time elapsed
\end{lstlisting}


% =================================================================
\section{Reflection and Conclusion}
% =================================================================

This lab demonstrated a systematic approach to code optimization. We achieved a total speedup of [e.g., 267.8x] over the Python baseline.

\subsection{Reflection: Dominant Architectural Factor}
[This is the most important part of the conclusion.]

The architectural factor that dominated performance was [Choose one and explain]:
\begin{itemize}
    \item \textbf{Data-Level Parallelism (SIMD):} The transition from scalar to AVX-512 operations provided the single largest jump in performance (a [Y]x speedup). This shows that for algorithms with regular, independent operations like S-W, exploiting SIMD is critical.
    \item \textbf{Memory-Hierarchy (Cache):} While SIMD was important, the cache-blocking strategy was essential to feed the vector units. Without it, the vectorized code was stalled on memory, and the speedup was only [Z]x. Therefore, managing the memory hierarchy was the true key.
    \item \textbf{Thread-Level Parallelism (Cores):} The problem was highly parallelizable, and the near-linear scaling up to [N] cores shows that the dominant factor was simply the amount of parallel hardware available.
\end{itemize}

[Elaborate on your choice]. For example: While multi-threading gave the final best result, the most \textit{architecturally interesting} optimization was the AVX-512 vectorization. It forced us to rethink the data dependencies and provided a [Y]x speedup on a single core, which is a testament to the power of data-level parallelism.

\subsection{Conclusion}
Through this lab, we successfully applied optimization principles to accelerate the Smith-Waterman algorithm significantly. The process highlighted the trade-offs at each step and the importance of a profiling-guided approach.


% =================================================================
\begin{thebibliography}{9}
% =================================================================

\bibitem{sw_wiki}
Wikipedia contributors. (2025, October). \textit{Smithâ€“Waterman algorithm}. Wikipedia.
\url{https://en.wikipedia.org/wiki/Smith%E2%80%93Waterman_algorithm}

\bibitem{sw_gfg}
GeeksforGeeks. (n.d.). \textit{Sequence Alignment Problem}.
\url{https://www.geeksforgeeks.org/dsa/sequence-alignment-problem/}

\bibitem{cpu_manual}
[Your CPU Manufacturer, e.g., Intel]. (2024). \textit{Intel 64 and IA-32 Architectures Optimization Reference Manual}.
[Add URL if you used it]

\end{thebibliography}

\end{document}